---
layout: post
title: Kafka设计原理
categories: [Kafka]
description: Kafka设计原理
keywords: Kafka
---

### 持久化

Kafka严重依赖文件系统来存储和缓存消息。很多人固有观念是硬盘慢，怀疑这样的存储结构无法提供好的性能。但是硬盘是快还是慢取决与如何使用硬盘，如果设计合理的硬盘结构是能够和互联网一样快。

6个7200rpm SATA RAID5阵列上顺序写的性能是600MB/s，但是随机写只有100k/s，相差6000倍。顺序读写都是可预测的，所以操作系统能够极大的优化。现代操作系统提供了预读和后写技术，读的时候预取多个块的数据，将小部分逻辑写合并为的物理写操作。下图为各种存储介质的顺序访问性能对比：

![各种存储介质的顺序访问性能对比](https://github.com/qinchunabng/qinchunabng.github.io/blob/master/images/posts/kafka/jacobs3.jpg?raw=true)

由于这个性能差异，现代操作系统大量的使用主存作为硬盘的缓存。现代操作会将所有的空闲内存作为硬盘缓存，代价只有内存回收时较小的性能损耗。所有硬盘读写都将通过这个统一的缓存。如果不使用直接I/O，不要轻易关闭这个功能。即使一个进程内存维护一个进程内部的数据缓存，这个数据可能在操作系统的页缓存中同样存在一份，这样就导致数据重复存储。

此外，kafka是运行在JVM中的，java的内存使用有以下两个问题：

1. 对象内存开销非常大，经常是要存储数据的两倍（甚至更大）。
2. 随着堆内存的增长，Java垃圾回收会变得越来越繁琐和缓慢。

由于这些因素，使用文件系统存依赖页缓存要优于维护内存中的缓存或者其他结构。通过自动访问空闲内存，我们至少将可用内存翻了一倍，同时通过紧凑的字节结构而不是单独的对象，可能还会增长一倍。这么会导致32GB机器会产生28-30GB缓存，但是不会有GC问题。此外即使重启也能保证缓存中的数据都是最新的需要的，然而如果是进程内的缓存就需要重建（10GB缓存可能需要10分钟），或者重启之后不预热缓存（可能会导致初期性能严重下降）。把缓存和文件系统的一致性交给操作系统来完成，也极大的简化的代码的业务逻辑，这比在进程中自己维护缓存要高效的多。因为硬盘线性读拥有更好的性能，所以每次读取硬盘的时候将一些有用的数据一起读取到缓存中这种预读的方式，是很高效的。

所以一个简单而高效的设计就是：相比在内存中维护一个缓存并且在缓存不够的时候再将数据写入文件系统，还不如颠倒过来，将所有的数据直接写入到文件系统的日志文件中，但不必写入硬盘，实际上就只是写入的内核的页缓存中。

在消息系统中一般每个消费者消费一个队列，消息的存储结构一般要么是BTree或者是支持随机访问的维护了消息元数据的数据结构。Btree是最通用的数据结构，支持消息系统中的各种事务的和非事务的语义。但是维护Btree的代价比较高，虽然Btree操作的时间复杂度是O(log N)。一般情况，O(log N)的时间复杂度等同于常量时间，但是对于磁盘操作来说并不是。磁盘寻址一般需要10ms一次，每个磁盘每个时刻只能执行一次寻址，即使是少量的磁盘寻道都有很高的开销。所以存储系统通过更快的缓存来避免磁盘的物理操作，而在固定大小缓存下随着数据的增长树结构的性能下降是超线性的（比如数据增长一倍，性能下降超过一倍）。

持久化队列构建在简单的读取和追加文件上，是常见的日志解决方案。这种机构优势是所有的操作都是O(1)，并且读不会阻塞其他的读或者写。还有一个巨大的优点是性能不再和数据大小有关系，这样服务器就可以充分利用一些便宜的低转速的1+TB SATA驱动器。虽然它们寻址性能很差，但是可以以1/3的价格和三倍的容量获得可以接受的读写性能。

可以访问近乎无效的磁盘空间，而且没有任何性能损失，这意味着我们可以提供一些在消息系统不常见的特性。例如，在kafka中，消息被消费之后并不是立马被删除，消息会被保留相当长一段时间（一般是一周），这就给了消费者很大灵活性。

### 效率

在前面一节我们讨论如何消除磁盘访问慢的问题，但是还有两个原因会影响性能：大量的小的I/O操作和过多的数据拷贝。

小的I/O经常发生在服务器和客户端之间的交互时或者服务器本身执行持久化操作的时候。为了避免这种情况的发生，kafka将多个消息组装一起发送而不是单个单个的发送，来分摊网络的开销。kafka依次的将消息块追加到日志中，消费者消费的时候一次获取多个消息。

这个简单的优化使得速度提升了几个数量级。批量操作会产生更多的网络包，更大的有序的磁盘操作，连续的内存块等等，这些使得kafka将随机的写操作转变为线性的写操作。

另外一个影响性能的是数据拷贝。在消息量少的这不是一个问题，但是在高负载下对性能会产生巨大的影响。为了避免这个问题，kafka定义一个标准的二进制消息格式，在生产者、broker、消费者之间是通用的，这样数据在它们之间传输就不需要修改。

消息日志是由broker维护的文件目录，每个日志文件的内容都是同样格式的且有序的消息集，并且被写入到磁盘中。维持这个通用的消息格式可以允许优化最重要的操作：日志块网络传输。现代unix操作系统对从页缓存传输数据到socket中提供了高度的优化方法，在linux中是通过sendfile系统调用完成的。

要理解sendfile的对性能的影响，先要理解一般是怎么从文件中传输数据到socket的：

1. 操作系统从磁盘中读取数据到内核空间的页缓存中
2. 应用程序将数据从内核空间读到用户空间的缓存中
3. 应用程序将数据写入到内核空间的socket缓冲区
4. 操作系统将数据从socket缓冲区发送NIC缓冲区，在这里数据被发送的网络中。

这个过程中有四次数据拷贝和两个系统调用，对性能会有很大的影响。使用sendfile，允许操作系统直接将数据从页缓存发送到网络，从而避免了重复的数据拷贝，最终只需要最后一次到NIC缓冲区的拷贝。

kafka中一个常见的消费方式是多个消费者消费一个topic。使用零拷贝，数据只需要从磁盘到页缓存拷贝一次，每次消费都被重用，而不是把数据保存内存中，每次读取时都拷贝到用户空间。这使得消息的消费速度能达到接近网络连接的限制。

通过页缓存和sendfile的组合，使kafka集群中消费者能够达到极佳的性能，因为读操作几乎不需要访问磁盘所有的数据都在缓存中。

由于TLS/SSL库的操作是在用户空间（kafka目前还不支持内核的SSL_sendfile），在SSL开启时不会使用sendfile。

java中零拷贝参考这篇[文章](https://qinchunabng.github.io/2023/03/16/java-zero-copy/)。

### 推模式和拉模式

Kafka使用了比较传统的设计，也是大多数消息采用的，生产者推送数据给broker，消费者从broker拉取消息。有些日志为中心的系统，采用的是推模式，主动给下游消费者推送数据。这两种方式各有有优缺点，推模式的系统缺点是难以控制给消费者推送数据的速率。一般来讲，消息者的目标是以最大的可能消费消息，但是在推模式下，消费者的消费速度很有可能会低于消息产生的速度，导致产生拒绝服务攻击。而拉模式消费取决于消费者自身，消费速度慢的时候就拉取速度就慢一些，消费地快就拉取的快。消费者可以通过一些补偿协议来表明消费者消费速度超过承受能力，继而调整消费速率。

另外一个拉模式的优点是消费者可以以一种比较好的方式批量拉取数据。推模式要么直接将消息推送给消费者，要么积累较多的数据一次推给消费者，但是无法确定下游的消费能否处理这些消息。为了低延迟，每次有消息的时候直接发送消息，但是在这样会造成性能的浪费。基于拉模式的设计总是拉取当前消费的位置之后所有的可用消息（或者固定数量的）。所以在批量处理的时候，延迟也较小。

拉模式的有一个缺点就是在broker没有数据的时候，消费者会一直空轮询，造成性能的浪费。为了避免这个问题，kafka在拉取的时候会带有一些参数，让消费者在长轮询的时候会一直阻塞，直到数据到来（或者等到有指定大小的数据到来时）。

### 消费者消费位置

追踪队列的消息位置，是消息系统性能的关键点之一。

大多数消息系统在将消息消费位置的元数据保存在broker中，所以当消息分发给消费者之后，broker要么直接直接记录下来，或者等待消费者发送确认。对于单服务器除了broker也没有其他的地方可以存储这个状态。因为大多数消息系统扩展性差，用这种数据结构存储是一个实用的选择。因为broker知道哪些消息消费了，就可以直接删除，可以保证存储的数据较小。

对于消费位置，broker和消费者很难达成一致。如果broker在消费分发给消费者之后直接记录消费位置，然后消费者消费消息失败，消息将会丢失。为了解决这个问题，很多消息系统添加确认机制，消息发送之后消费者之后，只是标记为已发送状态，等待消费者发送了确认消息之后，broker才记录消息为已消费状态。这种策略修复了丢消息的问题，但是又会导致新的问题。首先，如果消费已经处理了消息，但是没有发送确认消息，就会导致消息会被消费两次。另外一个问题就是性能问题，broker必须记录每个消息的多种状态（第一个状态为了防止消息第二次发送，第二个状态标记消息为已消费这样消息才能够被删除）。那么就有一些棘手的问题需要处理，比如消费发送但是没收到确认。

kafka使用不同方式处理这个问题。kafka的topic被分为了多个partition，每个消息在同一个时间只能被一个消费者组的一个消费者消费。这就意味着每个分区的消息者的消费位置只是一个整数，是下一个要消费消息的偏移量（offset）。这样消费的位置的状态就很简单，每个partition就一个数字。这样消息的确认机制就会很简单。

这样的处理方式还有另外一个好处就是，消费者可以随时调整消费的offset来重新消费。这违反消息的队列的规范，但是却带来一些好处。例如，如果一个消费者在消费了一些消息之后发现有bug，消费者可以在bug修复之后重新消费消息。

### 静态成员关系

静态成员关系是为了改善流式应用、消费者和其他一些构建在消费者组的再平衡协议之上应用的可用性。再平衡协议依赖于消费者组协调者的给消费者组的成员分配的实体ID。这些生成的ID是临时的，在消费者组成员重启或者加入时会改变。对于基于应用的消费者，这种动态的成员关系，在代码部署、配置更新或者重启时会导致大规模消费者实例的任务重新分配。对于一些应用，被打乱的任务需要很长时间才能恢复本地的状态，并且会导致的应用变为部分可用或者完全不可用。基于这个原因，kafka消费者组管理协议允许消费者提供一个永久的ID。组成员关系基于这些ID保持不变，因此就再平衡就不会触发。

如果你想使用静态成员关系，

- 升级broker和客户端的版本为2.3或2.3以上的版本，并且确认升级之后的broker使用`inter.broker.protocol.version`的协议版本是2.3或2.3以上。
- 通过`ConsumerConfig#GROUP_INSTANCE_ID_CONFIG`给消费者组的每个消费者设置一个唯一值。
- 对于kafka streams的应用，通过`ConsumerConfig#GROUP_INSTANCE_ID_CONFIG`给每个KafkaStreams实例设置一个唯一值。
  
如果你的broker版本低于2.3，但是在客户端设置了`ConsumerConfig#GROUP_INSTANCE_ID_CONFIG`，应用会检查broker版本，然后抛出一个UnsupportedException的异常。如果你设置了重复的ID值，broker检查机制会检查出来，然后通知ID重复的客户端，触发`org.apache.kafka.common.errors.FencedInstanceIdException`使客户端停止。

### 消息发送语义

kafka提供三种消息投递的保证：

- At most once: 消息可能会丢失，但是不会被重发。
- At least once: 消息不会丢失，但是会被重发。
- Exactly once: 每个消息只会被发送一次而且只有一次。

这就带来了两个问题：发送的消息的持久化保证和消费消息的持久化保证。

kafka中，发送的消息有提交的概念。一个消息一旦被提交，只要一个有写入消息的partition的broker存活，消息就不会丢失。如果一个生产者发送一个消息，并且遇上了网络错误，生产者是没有把办法确定这个错误是发送在消息被提交前还是提交后。这就类似向一个有自增列中的表中插入数据。

在0.11.0.0之前，如果生产者没有收到表示消息已经被提交的消息，生产者只能选择重发消息。这就提供了at-least-once的语义，因为如果原始的消息已经处理成功，消费重新发送会导致消息再次被写入日志中。从 0.11.0.0开始，kafka生产者支持幂等性发送选项，保证消息重新发送不会在日志中重复。为了实现消息发送的幂等性，broker会给每个生产者分配一个ID，生产者发送的每个不重复的消息会带有一个序号。从0.11.0.0开始，生产者支持使用类事务的语义将消息发送给多个topic partition：要么所有消息都写入成功，要么一个都不写入。这个主要用于多个kafka topic的精确一次处理。

不是所有的使用场景都要求这样强的保证。对延迟比较敏感的场景，生产者可以定义它需要的持久化等级。如果生产者定义需要等待消息被提供，发送成功需要10ms。生产者也可以通过配置，设置消息异步发送，或者只需要leader写入消息成功。

再从消费者视角来讨论这几种语义。所有副本都有同样的日志和同样的offset。消费者来控制消费的offset。如果消费者从不会挂机，可以将这个position保存在内存中，如果消费挂了我们希望另外一个消费者进程来接手，新的消费者进程需要选择一个合适的position来开始消费消息。消费者消费消息，更新消费的position有几种方式：
- 可以在读取消息之后，在日志中保存消费的位置，然后再消费消息。这种情况下，消费者进程在保存消费位置之后，但是还没有保存消费的输出结果时，可能会挂掉。这种情况，接管的消费者进程会从保存的消费位置继续消费，就会导致有些消息没有被真正的处理。这就对应了最多一次的语义，因为消费者挂掉之后，消息可能就不会被处理。
- 读取消息，处理消息，最后再保存消费的位置。这种情况下，可能会出现消息已经处理了，但是还没有保存消费位置，消费者进程挂了。这时，接管的消费者进程处理的前面一部分消息可能是已经被处理过的。这就对应at-least-once的语义。在大多数场景，消息都有一个主键值，可以用来实现幂等性更新（收到同样的消息的两次，只需要覆盖写入的记录）。

那么什么是exactly-once语义？当从一个kafka topic消费消息之后，生产消费发送到另外一个topic，我们可以利用0.11.0.0中新加的事务来处理。消费者消费的位置是存储在一个topic中消息，所以我们可以将写入offset到kafka和发送结果处理数据到另外一个topic放到一个事务中。如果是被终止，消费者的position会恢复到旧的位置，并且生成数据到另外一个topic消息对其他的消费不可见，这个取决于它的隔离级别。在默认read_uncommitted的隔离级别下，所有消息对消费者都可见，即使这些消息是属于一个中断的事务。但是在read_committed下，消费只能看到已经提交的事务的消息。

当写入到外部系统，消费者消费位置和消费输出结果存储要处理好。一般都是在存储消费位置和存储消费输出结果引入两阶段提交。但是也可以将消费位置和消费输出结果存储到同样的地方，通过这种简单方式解决。这种处理方式更好，因为很多存储输出结果的存储系统不支持两阶段提交。

### 副本

kafka会将日志从topic partition的leader中配置的到follwer中，这样就能实现自动故障转移，当集群中有节点发证故障时整个集群仍然可用。

副本是以topic partition为单位的。在kafka中，每个partition都有一个leader和0个或多个follower，replication factor是包含了leader的总副本数。所有的写操作都发送到leader，读操作发送到leader和follower。如果partition的数量比broker多，leader会平均分布在broker上。follower上的数据和leader的数据是一样的，都有同样的offset和同样的顺序（当然也会存在leader中的消息日志还没有同步到follower的情况）。follower从leader消费消费消息就和普通的消费者一样，会把消息写入到follower的日志文件中。

在大多数分布式系统中，自动故障转移需要给节点是否存活一个准确的定义。在kafka中，有一个叫controller的特殊节点，用来管理集群节点的注册。判断broker是否存活需要满足以下两个条件：

1. broker要与controller维护一个活跃的会话，用于接收元数据的更新。
2. follower节点要从leader节点同步数据，并且不能落后leader太多。

什么活跃的会话取决于集群配置。对于KRaft集群，保持会话活跃需要发送心跳到controller。如果controller超过`broker.session.timeout.ms`没有收到心跳，就认为这个节点下线了。

对于使用zookeeper的集群，是否活跃是通过broker初始化zookeeper会话时创建的临时节点。如果broker超过`zookeeper.session.timeout.ms`每个发送心跳给zookeeper，临时节点会被删除。controller会监听到临时节点删除，然后标记这个broker为下线。

满足上面两个条件的节点我们称之为同步节点。leader与同步的节点（ISR）保持连接。如果两个条件任何一个不满足，broker将会从ISR移除。例如，如果一个follower宕机了，controller会通过会话检测出来，然后就会从ISR中移除这个broker。另外一种情况是，如果follower落后leader太多，但是会话却是活跃的，leader也会从ISR中将它移除。follower落后broker的最大时间是通过`replica.lag.time.max.ms`配置的，如果follower超过配置的最大时间没有追上leader，就会被从ISR中移除。

在分布式系统术语中，只处理节点停止工作，过了段时间又恢复（可能节点并不知道它们出现了问题）这种失败恢复的模型。kafka不会处理因为BUG或者什么原因导致节点出现错误的响应而导致的拜占庭式故障。

当partition的ISR中的所有的副本都写入日志后，消息可以认为是已经commit，只有已提交的消息会被分发到消费者。这意味消费者不必担心会看到因为leader宕机而导致而丢失的消息。生产者可以配置是否等待消息提交，根据消息的延迟和持久化来权限。

### leader选举

当leader挂了之后，需要从follower中选择一个作为新的leader，但是follower的数据落后于leader，需要选择一个包含最新数据的follower。复制算法必须保证，如果一个消息被提交，并且leader挂了，新选举出来的leader必须也有这个消息。所以这里需要做一个权衡：如果leader写入消息之后，等待更多的follower消息同步确认，在选举的时候就有较多符合条件可以作为leader的follower。一个比较通用的方式是，使用过半选举机制来选举leader和判断消息是否提交。但是kafka不是这么做的。假如现在又2f+1个副本，那么在消息提交之前，必须有f+1副本都收到消息，如果是选举leader的话，也是从这f+1个副本中选举，来保证新选举出来的leader有所有已提交的消息。

对于符合条件的follower，可以参加leader选举的follower，就称为Quorum。

过半选举还有一个优点就是：延迟是取决是最快的服务节点。比如：如果现在有三个副本，延迟是取决于最快的follower，不是最慢的。

过半选举的缺点是，如果挂掉的节点多了，就无法进行选举。如果要支持1个节点宕机，那就需要三个副本，如果要支持2个节点宕机，就需要5个副本。在实际使用中，只能允许1个节点宕机是不够的，但是部署5个节点，每次就需要写5次，就相当于5倍的磁盘空间的要求和1/5的吞吐量，数据量大的话是非常不划算的。这也是为什么过半选举算法一般都是用在像zookeeper这种共享集群配置中间件上，在数据存储为主的系统不会使用。

kafka在选择quorum的时候，做了一些细微的改变。kafka动态地维护了一个ISR集，就是数据与leader一致的follower，只有ISR中成员才能被选举为leader。一个partition的写操作，需要所有的ISR接收到之后才算是commit。ISR集在改变的时候会保存到集群元数据中。通过ISR，如果有f+1个副本，kafka允许f个节点宕机，并且不会出现commit的消息丢失。

在实际使用过程中，要允许f个节点失效，过半选举和ISR的方式在提交消息之前，都需要等待同样数量副本的确认（例如：允许一个节点失效，过半选举需要3个副本和一个确认，ISR的方式需要两个副本和一个确认）。过半选举的优点的是不需要等待最慢的节点。

kafka另外一个特征是恢复失效的节点不需要所有的数据是完整的。复制算法一般都需要依赖文档的存储，但是在这种方式存在两个主要的问题。首先，磁盘的错误是很常见的，经常会导致数据不完整。其次，即使不存在磁盘错误，我们也不希望每次执行写操作都使用fsync来保证数据一致，这样会使性能下降2到3个维度。kafka要求副本在重新加入集群之前，必须完全同步数据。

要做到数据不丢失，必须保证至少有ISR中至少有一个副本，如果ISR中一个副本节点都没有了，这点就无法保证。

如果所有的副本节点都挂了，就只有两个选择：
1. 等待ISR中的一个副本恢复，并且选举这个副本为leader。
2. 选择第一个恢复的副本（不一定在ISR中）为leader。
   
这是对可用性和一致性的权衡。如果我们等待ISR中的副本恢复，kafka在ISR副本没有恢复前会一直不可用。如果这些副本被摧毁或者数据丢失了，kafka就无法恢复。如果允许非ISR的节点恢复后被选为leader，就无法保证这个节点拥有所有已提交的数据。从0.11.0.0开始，kafka默认选择的使第一个策略来保证数据的一致性。可以通过`unclean.leader.election.enable`配置允许选择非ISR节点为leader，一般是在可用性高于一致性的情况下使用。

这种问题不光在kafka中存在，在所有基于quorum方案中都有。例如：在过半选举的机制中，如果超过一半的服务器都出现了问题，要么选择丢失所有的数据，要么选择剩下服务器作为数据源。

### 可用性和可靠性的保证

在kafka中，生产者写入数据时，可以配置等待消息被0,1或者all(-1)个副本确认。all(-1)并不是被所有的副本确认，只需要保证ISR中所有的副本确认。例如，如果一个topic配置了两个副本，其中一个已经挂了（只有一个在ISR中），并且acks=all，写入数据也会成功。但是，如果剩下的副本也挂了，这些写操作可能会丢失。虽然这在最大限度上保证了可用性，但是有些场景下，可靠性比可用性更重要。因此，kafka提供了两个topic级的配置，用来设置优先考虑可靠性：

1. 关闭unclean leader的选举。如果所有的副本都挂了，这个partition就会一直不可用，只到最近的leader恢复。
2. 定义最小的ISR数量。partition只会在ISR数量大于等于配置的最小的ISR数时，才会接受写入操作，避免消息写入唯一的副本后，然后这个副本挂了导致数据丢失。这个配置只有在生产者acks=all才有效，这个配置是在一致性和可靠性的一个折中的配置。增加这个配置值，可以提供更高的一致性，因为消息会写入到更多的副本中，减少了数据丢失的可能性。但是这会减小可用性，因为如果ISR数量小于最小的ISR配置值，partition就无法写入。

### 副本管理

上面讨论的都是针对单个topic partition的日志复制的问题，但是kafka集群往往需要管理成百上千个partition。kafka会通过轮询的方式将partition平均分配到所有的集群节点，避免部分集群节点负载过高。

leader的选举过程中kafka集群是不可用的，所有需要优化leader的选举过程，减少leader选举时间。kafka集群中有一个叫controller的特殊节点，它的职责就是管理broker的注册。如果controller检测到broker失效，就会从ISR中选择一个作为新的leader，这样就可以又快又省性能的方式完成多个partition的leader选举。如果controller挂了，会选举另外一个新的controller。



